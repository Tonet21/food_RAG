{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1789966,"sourceType":"datasetVersion","datasetId":1063627}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:38:31.765142Z","iopub.execute_input":"2025-03-14T17:38:31.765487Z","iopub.status.idle":"2025-03-14T17:38:33.035136Z","shell.execute_reply.started":"2025-03-14T17:38:31.765456Z","shell.execute_reply":"2025-03-14T17:38:33.033842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_path = \"/kaggle/input/foodcom-recipes-and-reviews/recipes.csv\"\n\ndf = pd.read_csv(csv_path)\n\n#print(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:39:31.498108Z","iopub.execute_input":"2025-03-14T17:39:31.498546Z","iopub.status.idle":"2025-03-14T17:39:51.537173Z","shell.execute_reply.started":"2025-03-14T17:39:31.498511Z","shell.execute_reply":"2025-03-14T17:39:51.535923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport html\n\n# Ensure column names are clean\ndf.columns = df.columns.str.strip()\n\n# Select only columns that exist in df\nselected_columns = [\"RecipeId\", \"Name\", \"RecipeIngredientQuantities\", \"RecipeIngredientParts\", \n                    \"Calories\", \"FatContent\", \"CarbohydrateContent\", \"FiberContent\", \n                    \"ProteinContent\", \"RecipeInstructions\"]\n\nexisting_columns = [col for col in selected_columns if col in df.columns]\ndf_subset = df[existing_columns].copy()  # Use .copy() to prevent modification issues\n\nprint(\"Before applying function:\", df_subset.columns.tolist())\n\n# Function to clean 'c(...)' format\ndef clean_c_format(text):\n    if not isinstance(text, str) or text.strip() == \"\":\n        return \"\"  # Always return a string\n    \n    match = re.match(r'^c\\((.*)\\)$', text.strip())\n    cleaned = match.group(1) if match else text.strip()\n    \n    cleaned_parts = re.findall(r'\"(.*?)\"', cleaned)\n    cleaned = \" \".join(cleaned_parts) if cleaned_parts else cleaned\n\n    return html.unescape(cleaned) or \"\"\n\n# Apply cleaning function\nfor col in [\"RecipeIngredientQuantities\", \"RecipeIngredientParts\", \"RecipeInstructions\"]:\n    if col in df_subset.columns:\n        df_subset[col] = df_subset[col].apply(clean_c_format)\n\nprint(\"After applying function:\", df_subset.columns.tolist())\nprint(df_subset.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:08:45.449181Z","iopub.execute_input":"2025-03-14T18:08:45.449537Z","iopub.status.idle":"2025-03-14T18:09:03.801256Z","shell.execute_reply.started":"2025-03-14T18:08:45.449503Z","shell.execute_reply":"2025-03-14T18:09:03.800192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = df_subset.head(500).to_dict(orient=\"records\")\nprint(data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:10:19.803273Z","iopub.execute_input":"2025-03-14T18:10:19.803640Z","iopub.status.idle":"2025-03-14T18:10:19.813464Z","shell.execute_reply.started":"2025-03-14T18:10:19.803607Z","shell.execute_reply":"2025-03-14T18:10:19.812099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = [\"RecipeId\", \"Name\"]\ndf_units = df[columns].head(500)  # Get first 500 rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:57:32.443053Z","iopub.execute_input":"2025-03-14T17:57:32.443417Z","iopub.status.idle":"2025-03-14T17:57:32.457452Z","shell.execute_reply.started":"2025-03-14T17:57:32.443386Z","shell.execute_reply":"2025-03-14T17:57:32.455949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n# Popular units pattern\nPOPULAR_UNITS = [\"cups\", \"cup\", \"tablespoons\", \"tablespoon\", \"tbsp\", \"tsp\", \"teaspoons\", \"teaspoon\", \n                 \"grams\", \"gram\", \"kilograms\", \"kilogram\", \"kg\", \"ounces\", \"ounce\", \"oz\",\n                 \"pounds\", \"pound\", \"lb\", \"milliliters\", \"milliliter\", \"ml\", \"liters\", \"liter\", \"l\"]\nUNITS_PATTERN = r\"^(\" + \"|\".join(POPULAR_UNITS) + r\")\"\n\n# Function to modify name for URL\ndef modify_data(rid, name):\n    return rid, name.lower().replace(\" \", \"-\")\n\n# Function to extract units\ndef extract_unit(text):\n    if not isinstance(text, str) or text.strip() == \"\":\n        return None  \n\n    text = text.lower().strip()\n    matches = re.findall(UNITS_PATTERN, text)  # Find all unit matches\n    return max(matches, key=len) if matches else None  # Pick longest match (plural first)\n\n# Function to scrape and clean units\ndef scrape_units(df_unit):\n    unit_data = []\n    \n    for _, row in df_unit.iterrows():\n        rid, name = modify_data(row[\"RecipeId\"], row[\"Name\"])\n        url = f'https://www.food.com/recipe/{name}-{rid}'\n        \n        try:\n            response = requests.get(url)\n            response.raise_for_status()  \n        except requests.exceptions.RequestException as e:\n            print(f\"Failed to fetch {url}: {e}\")\n            continue\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Extract units\n        cleaned_units = []\n        for li in soup.find_all('li', style=\"display: contents\"):\n            ingredient_text = li.find('span', class_='ingredient-text')\n            if ingredient_text:\n                raw_unit = ingredient_text.get_text(strip=True).split(maxsplit=1)[0]  # First word\n                cleaned_unit = extract_unit(raw_unit)  # Clean using regex\n                if cleaned_unit:\n                    cleaned_units.append(cleaned_unit)\n        \n        unit_data.append({\"RecipeId\": rid, \"Units\": cleaned_units})\n    \n    return pd.DataFrame(unit_data)\n\n# Run the function\ndf_units = scrape_units(df_units)\n\n# Display first rows\nprint(df_units.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:57:34.751497Z","iopub.execute_input":"2025-03-14T17:57:34.751865Z","iopub.status.idle":"2025-03-14T18:01:55.316292Z","shell.execute_reply.started":"2025-03-14T17:57:34.751836Z","shell.execute_reply":"2025-03-14T18:01:55.315025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"units = df_units.to_dict(orient=\"records\")\n\nprint(units[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:05:28.728404Z","iopub.execute_input":"2025-03-14T18:05:28.728768Z","iopub.status.idle":"2025-03-14T18:05:28.745613Z","shell.execute_reply.started":"2025-03-14T18:05:28.728739Z","shell.execute_reply":"2025-03-14T18:05:28.744069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = [ {k.lower(): v for k, v in row.items()} for row in data]\nunits = [ {k.lower(): v for k, v in row.items()} for row in units]\nprint(units[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:24:02.155224Z","iopub.execute_input":"2025-03-14T18:24:02.155634Z","iopub.status.idle":"2025-03-14T18:24:02.166105Z","shell.execute_reply.started":"2025-03-14T18:24:02.155598Z","shell.execute_reply":"2025-03-14T18:24:02.164858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install supabase\n\n\nfrom supabase import create_client\nfrom kaggle_secrets import UserSecretsClient\n\nurl = UserSecretsClient().get_secret(\"SUPABASE_URL\")\napi_key = UserSecretsClient().get_secret(\"SUPABASE_KEY\")\nsupabase = create_client(url, api_key)\n\nresponse = (\n    supabase.table(\"foodcom\")\n    .insert(data)\n    .execute()\n)\n\nresponse = (\n    supabase.table(\"units\")\n    .insert(units)\n    .execute()\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:26:12.420760Z","iopub.execute_input":"2025-03-14T18:26:12.421172Z","iopub.status.idle":"2025-03-14T18:26:18.073839Z","shell.execute_reply.started":"2025-03-14T18:26:12.421135Z","shell.execute_reply":"2025-03-14T18:26:18.072604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}